@article{Girolami2008,
abstract = {Nonlinear dynamic systems such as biochemical pathways can be represented in abstract form using a number of modelling formalisms. In particular differential equations provide a highly expressive mathematical framework with which to model dynamic systems, and a very natural way to model the dynamics of a biochemical pathway in a deterministic manner is through the use of nonlinear ordinary or time delay differential equations. However if, for example, we consider a biochemical pathway the constituent chemical species and hence the pathway structure are seldom fully characterised. In addition it is often impossible to obtain values of the rates of activation or decay which form the free parameters of the mathematical model. The system model in many cases is therefore not fully characterised either in terms of structure or the values which parameters take. This uncertainty must be accounted for in a systematic manner when the model is used in simulation or predictive mode to safeguard against reaching conclusions about system characteristics that are unwarranted, or in making predictions that are unjustifiably optimistic given the uncertainty about the model. The Bayesian inferential methodology provides a coherent framework with which to characterise and propagate uncertainty in such mechanistic models and this paper provides an introduction to Bayesian methodology as applied to system models represented as differential equations. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Girolami, Mark},
doi = {10.1016/j.tcs.2008.07.005},
file = {:Users/jp2011/Dropbox/phd/mres/machine-learning/Girolami - 2008 - Bayesian inference for differential equations.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Bayesian statistics,Biochemical pathway models,Differential equations},
number = {1},
pages = {4--16},
publisher = {Elsevier B.V.},
title = {{Bayesian inference for differential equations}},
url = {http://dx.doi.org/10.1016/j.tcs.2008.07.005},
volume = {408},
year = {2008}
}
@article{Figueiredo2003,
author = {Figueiredo, A T and Member, Senior},
file = {:Users/jp2011/Dropbox/phd/papers/Figueiredo, Member - 2003 - Adaptive Sparseness for Supervised Learning.pdf:pdf},
number = {9},
pages = {1150--1159},
title = {{Adaptive Sparseness for Supervised Learning}},
volume = {25},
year = {2003}
}
@book{P.Murphy1991,
abstract = {Some of the most remarkable issues related to interharmonics observed from a probabilistic perspective are presented. Attention is firstly devoted to interharmonic frequency and amplitude variability. Starting from the basic mathematical and computational aspects of probabilistic harmonic models, the difficulties to include interharmonics are discussed with particular attention to the problem of the frequency resolution and of the computational burden. Then, simulation and measurement aspects are discussed, also showing some numerical and experimental results.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{P. Murphy}, Kevin},
booktitle = {Machine Learning: A Probabilistic Perspective},
doi = {10.1007/SpringerReference_35834},
eprint = {0-387-31073-8},
file = {:Users/jp2011/Dropbox/phd/textbooks/(Adaptive Computation and Machine Learning) Kevin P. Murphy-Machine Learning{\_} A Probabilistic Perspective-The MIT Press (2012).pdf:pdf},
isbn = {9780262018029},
issn = {0262018020},
pages = {73--78,216--244},
pmid = {20236947},
title = {{Machine Learning: A Probabilistic Perspective}},
url = {http://link.springer.com/chapter/10.1007/978-94-011-3532-0{\_}2},
year = {1991}
}
@article{Berger2004,
abstract = {Statistics has struggled for nearly a century over the issue of whether the Bayesian or frequentist paradigm is superior. This debate is far from over and, indeed, should continue, since there are fundamental philosophical and pedagogical issues at stake. At the methodological level, however, the debate has become considerably muted, with the recognition that each approach has a great deal to contribute to statistical practice and each is actually essential for full development of the other approach. In this article, we embark upon a rather idiosyncratic walk through some of these issues.},
author = {Berger, J. O. and Bayarri, M. J.},
doi = {10.1214/088342304000000116},
file = {:Users/jp2011/Dropbox/phd/mres/Fundamentals of Statistical Inference/background reading/The Interplay of Bayesian and Frequentist Analysis.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {admissibility,and phrases,bayesian model checking,chical models,condi-,confidence intervals,consistency,coverage,design,hierar-,nonparametric bayes,objective bayesian methods,p -values,reference priors,testing,tional frequentist},
number = {1},
pages = {58--80},
title = {{The Interplay of Bayesian and Frequentist Analysis}},
url = {http://projecteuclid.org/euclid.ss/1089808273},
volume = {19},
year = {2004}
}
@article{Efron1998,
abstract = {Fisher is the single most important figure in 20th century statistics. This talk examines his influence on modern statistical thinking, trying to predict how Fisherian we can expect the 21st century to be. Fisher's philosophy is characterized as a series of shrewd compromises between the Bayesian and frequentist viewpoints, augmented by some unique characteristics that are particularly useful in applied problems. Several current research topics are examined with an eye toward Fisherian influence, or the lack of it, and what this portends for future statistical developments. Based on the 1996 Fisher lecture, the article closely follows the text of that talk.},
author = {Efron, Bradley},
doi = {10.1214/ss/1028905930},
file = {:Users/jp2011/Dropbox/phd/mres/Fundamentals of Statistical Inference/background reading/R. A. Fisher in the 21st Century.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Bayes,bootstrap,confidence intervals,empirical Bayes,fiducial,frequentist,model selection,statistical inference},
pages = {95--122},
title = {{R. A. Fisher in the 21st century (Invited paper presented at the 1996 R. A. Fisher Lecture)}},
url = {http://projecteuclid.org/euclid.ss/1028905930},
volume = {13},
year = {1998}
}
@article{Reid2015,
abstract = {Statistical theory aims to provide a foundation for studying the collection and interpretation of data, a foundation that does not depend on the particular details of the substantive field in which the data are being considered. This gives a systematic way to approach new problems, and a common language for summarising results; ideally, the foundations and common language ensure that statistical aspects of one study, or of several studies on closely related phenomena, can be broadly accessible. We discuss some principles of statistical inference, to outline how these are, or could be, used to inform the interpretation of results, and to provide a greater degree of coherence for the foundations of statistics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Reid, Nancy and Cox, David R.},
doi = {10.1111/insr.12067},
eprint = {arXiv:1011.1669v3},
file = {:Users/jp2011/Dropbox/phd/mres/Fundamentals of Statistical Inference/background reading/On Some Principles of Statistical Inference.pdf:pdf},
isbn = {9780511813559},
issn = {17515823},
journal = {International Statistical Review},
keywords = {Ancillary,Bayesian, conditional, likelihood,Models, p-values,Sufficient},
number = {2},
pages = {293--308},
pmid = {18450045},
title = {{On Some Principles of Statistical Inference}},
volume = {83},
year = {2015}
}
@article{Berger2003,
abstract = {Ronald Fisher advocated testing using p-values, Harold Jeffreys proposed use of objective posterior probabilities of hypotheses and Jerzy Neyman recommended testing with fixed error probabilities. Each was quite critical of the other approaches. Most troubling for statistics and science is that the three approaches can lead to quite different practical conclusions. This article focuses on discussion of the conditional frequentist approach to testing, which is argued to provide the basis for a methodological unification of the approaches of Fisher, Jeffreys and Neyman. The idea is to follow Fisher in using p-values to define the "strength of evidence" in data and to follow his approach of conditioning on strength of evidence; then follow Neyman by computing Type I and Type II error probabilities, but do so conditional on the strength of evidence in the data. The resulting conditional frequentist error probabilities equal the objective posterior probabilities of the hypotheses advocated by Jeffrey},
author = {Berger, James O.},
doi = {10.1214/ss/1056397485},
file = {:Users/jp2011/Dropbox/phd/mres/Fundamentals of Statistical Inference/background reading/Could Fisher, Jeffreys and Neyman Have Agreed on Testing.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,conditional testing,fisher,for many,ii error probabilities,is quite different,jeffreys and neyman dis-,p -values,posterior probabilities of hypotheses,the situation in testing,type i and type,types of testing},
number = {1},
pages = {1--32},
title = {{Could Fisher, Jeffreys and Neyman Have Agreed on Testing?}},
url = {http://projecteuclid.org/euclid.ss/1056397485},
volume = {18},
year = {2003}
}
@unpublished{Marcus,
author = {Marcus, Gary},
file = {:Users/jp2011/Dropbox/phd/papers/Marcus - Unknown - Deep Learning A Critical Appraisal.pdf:pdf},
pages = {1--27},
title = {{Deep Learning : A Critical Appraisal}}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
annote = {Things to follow up:
- Bayesian model averaging: different from ensembles},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
file = {:Users/jp2011/Dropbox/phd/mres/machine-learning/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {78},
pmid = {1000183096},
primaryClass = {cs},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Taylor2017,
abstract = {There are a variety of challenges that come with producing a large number of forecasts across a variety time series. Our approach to fore-casting at scale is a combination of configurable models and thorough analyst-in-the-loop performance analysis. We present a forecasting ap-proach based on a decomposable model with interpretable parameters that can be intuitively adjusted by the analyst. We describe performance analysis that we use compare and evaluate forecasting procedures, as well as automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable forecasting of a large variety of business time series.},
author = {Taylor, Sean J and Letham, Benjamin},
doi = {10.7287/peerj.preprints.3190v1},
file = {:Users/jp2011/Dropbox/phd/papers/Taylor, Letham - 2017 - Forecasting at Scale.pdf:pdf},
issn = {2167-9843},
journal = {PeerJ Preprints},
keywords = {nonlinear regression,statistical practice,time series},
pages = {1--17},
title = {{Forecasting at Scale}},
year = {2017}
}
@article{DeMicheaux2009,
abstract = {Also see http://en.wikipedia.org/wiki/Convergence{\_}in{\_}distribution},
annote = {Very good for understanding the difference between convergence in probability and convergence almost surely9},
author = {{De Micheaux}, Pierre Lafaye and Liquet, Benoit},
doi = {10.1198/tas.2009.0032},
file = {:Users/jp2011/Dropbox/phd/papers/De Micheaux, Liquet - 2009 - Teacher's corner understanding convergence concepts A visual-minded and graphical simulation-based approach.pdf:pdf},
issn = {00031305},
journal = {American Statistician},
keywords = {Convergence almost surely,Convergence in law,Convergence in probability,Convergence in rth mean,Dynamic graphics,Monte carlo simulation,R language,Visualization},
number = {2},
pages = {173--178},
title = {{Teacher's corner understanding convergence concepts: A visual-minded and graphical simulation-based approach}},
volume = {63},
year = {2009}
}
@article{Scott2016,
abstract = {Abstract A useful definition of “ big data ” is data that is too big to comfortably process on a single machine, either because of processor, memory, or disk bottlenecks. Graphics processing units can alleviate the processor bottleneck, but memory or disk bottlenecks ... $\backslash$n},
annote = {I have notes on Overleaf},
author = {Scott, Steven L. and Blocker, Alexander W. and Bonassi, Fernando V. and Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
doi = {10.1080/17509653.2016.1142191},
file = {:Users/jp2011/Dropbox/phd/mres/Computational Statistics/CW5/Bayes and big data the consensus Monte Carlo algorithm.pdf:pdf},
issn = {17509661},
journal = {International Journal of Management Science and Engineering Management},
keywords = {Bayesian inference,Big data,Distributed computing,Embarrassingly parallel,Markov chain Monte Carlo},
number = {2},
pages = {78--88},
publisher = {Taylor {\&} Francis},
title = {{Bayes and big data: The consensus monte carlo algorithm}},
url = {http://dx.doi.org/10.1080/17509653.2016.1142191},
volume = {11},
year = {2016}
}
@article{Harville1977,
author = {Harville, D. A},
file = {:Users/jp2011/Dropbox/phd/papers/Harville - 1977 - Maximum Likelihood Approaches to Variance Components Estimation and to Related Problems.pdf:pdf},
journal = {Jasa},
keywords = {hood,maxi-,maximum likeli-,mixed linear models,mum likelihood computations,restricted maximum likelihood,variance component estimation},
number = {December},
pages = {320--337},
title = {{Maximum Likelihood Approaches to Variance Components Estimation and to Related Problems}},
volume = {72},
year = {1977}
}
@article{Corbeil1976,
author = {Corbeil, R.{\~{}}R. and Searle, S.{\~{}}R.},
file = {:Users/jp2011/Dropbox/phd/papers/Restricted Maximum Likelihood Estimation 10.1080{\_}00401706.1976.10489397.pdf:pdf},
journal = {Technometrics},
number = {December},
pages = {31--38},
title = {{Restricted Maximum Lielihood Estimation of Variance Components in the Mixed Model}},
volume = {18},
year = {1976}
}
@article{Box1964,
author = {Box, G. E. P. and Cox, D. R},
doi = {10.1079/IVPt200454()IN},
file = {:Users/jp2011/Dropbox/phd/papers/Box, Cox - 1964 - An Analysis of Transformations.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
keywords = {analysis of variance,choice of variables,crossvalidation,doublecross,modelmix,multiple regression,prediction,prescription,univariate estimation},
number = {2},
pages = {211--252},
title = {{An Analysis of Transformations}},
volume = {26},
year = {1964}
}
@article{Neary2017,
author = {Neary, Peter and Overman, Henry and Roberts, Kevin and Sturm, Daniel and Young, Alastair},
doi = {10.1093/qje/qjx020.Advance},
file = {:Users/jp2011/Dropbox/phd/papers/Neary et al. - 2017 - THE BENEFITS OF FORCED EXPERIMENTATION STRIKING EVIDENCE FROM THE LONDON UNDERGROUND NETWORK.pdf:pdf},
journal = {The Quarterly Journal of Economics},
number = {October},
pages = {2019--2055},
title = {{THE BENEFITS OF FORCED EXPERIMENTATION: STRIKING EVIDENCE FROM THE LONDON UNDERGROUND NETWORK}},
volume = {132},
year = {2017}
}
@techreport{Feynman1966,
author = {Feynman, Richard},
booktitle = {National Science Teachers Association},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Feynman - 1966 - What is Science(2).pdf:pdf},
number = {6},
pages = {313--320},
title = {{What is Science?}},
url = {http://profizgl.lu.lv/pluginfile.php/32795/mod{\_}resource/content/0/WHAT{\_}IS{\_}SCIENCE{\_}by{\_}R.Feynman{\_}1966.pdf?t=1{\&}cn=ZmxleGlibGVfcmVjcw{\%}3D{\%}3D{\&}refsrc=email{\&}iid=d403f77ed87f4a02a9c79853887e1c95{\&}fl=4{\&}uid=906872088{\&}nid=244+281088008},
volume = {7},
year = {1966}
}
@article{Polson2017,
abstract = {Deep learning is a form of machine learning for nonlinear high di-mensional pattern matching and prediction. By taking a Bayesian probabilistic perspective, we provide a number of insights into more efficient algorithms for op-timisation and hyper-parameter tuning. Traditional high-dimensional data reduc-tion techniques, such as principal component analysis (PCA), partial least squares (PLS), reduced rank regression (RRR), projection pursuit regression (PPR) are all shown to be shallow learners. Their deep learning counterparts exploit mul-tiple deep layers of data reduction which provide predictive performance gains. Stochastic gradient descent (SGD) training optimisation and Dropout (DO) reg-ularization provide estimation and variable selection. Bayesian regularization is central to finding weights and connections in networks to optimize the predictive bias-variance trade-off. To illustrate our methodology, we provide an analysis of international bookings on Airbnb. Finally, we conclude with directions for future research.},
author = {Polson, Nicholas G and Sokolov, Vadim},
doi = {10.1214/17-BA1082},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Polson, Sokolov - 2017 - Deep Learning A Bayesian Perspective.pdf:pdf},
journal = {Bayesian Analysis},
keywords = {Artificial Intelligence,Bayesian hierarchical models,LSTM models,TensorFlow,deep learning,machine learning,pattern matching,prediction},
number = {4},
pages = {1275--1304},
title = {{Deep Learning: A Bayesian Perspective}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.ba/1510801992},
volume = {12},
year = {2017}
}
@article{Mohamed2015,
abstract = {I've taken to writing this series of posts on a statistical view of deep learning with two principal motivations in mind. The first was as a personal exercise to make con- crete and to test the limits of the way that I think about and use deep learning in my every day work. The second, was to highlight important statistical connections and im- plications of deep learning that I have not seen made in the popular courses, reviews and books on deep learn- ing, but which are extremely important to keep in mind. This document forms a collection of these essays originally posted at blog.shakirm.com.},
author = {Mohamed, Shakir},
number = {July},
pages = {1--31},
title = {{A Statistical View of Deep Learning}},
url = {http://blog.shakirm.com/ml-series/a-statistical-view-of-deep-learning/ blog.shakirm.com},
year = {2015}
}
@article{Stone2014,
abstract = {In the early morning hours of June 1, 2009, during a flight from Rio de Janeiro to Paris, Air France Flight AF 447 disappeared during stormy weather over a remote part of the Atlantic carrying 228 passengers and crew to their deaths. After two years of unsuccessful search, the authors were asked by the French Bureau d'Enquˆ etes et d'Analyses pour la s´ ecurit´ e de l'aviation to develop a probability dis- tribution for the location of the wreckage that accounted for all infor- mation about the crash location as well as previous search efforts. We used a Bayesian procedure developed for search planning to pro- duce the posterior target location distribution. This distribution was used to guide the search in the third year, and the wreckage was found with one-week of search. In this paper we discuss why Bayesian analysis is ideally suited to solving this problem, review previous non-Bayesian efforts, and describe the methodology used to produce the posterior probability distribution for the location of the wreck},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.4720v1},
author = {Stone, Lawrence D. and Keller, Colleen M. and Kratzke, Thomas M. and Strumpfer, Johan P.},
doi = {10.1214/13-STS420},
eprint = {arXiv:1405.4720v1},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Stone et al. - 2014 - Search for the Wreckage of Air France Flight AF 447.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {AF 447, Bayesian, particle filter,,and phrases},
number = {1},
pages = {69--80},
title = {{Search for the Wreckage of Air France Flight AF 447}},
url = {http://projecteuclid.org/euclid.ss/1399645730},
volume = {29},
year = {2014}
}
@article{Goldstein1996,
abstract = {In the light of an increasing interest in the accountability of public institutions, this paper sets out the statistical issues involved in making quantitative comparisons between institutions in the areas of health and education. We deal in detail with the need to take account of model-based uncertainty in making comparisons. We discuss the need to establish appropriate measures of institutional `outcomes' and base-line measures and the need to exercise care and sensitivity when interpreting apparent differences. The paper emphasizes that statistical methods exist which can contribute to an understanding of the extent and possible reasons for differences between institutions. It also urges caution by discussing the limitations of such methods.},
author = {Goldstein, Harvey and Spiegelhalter, David J},
doi = {10.2307/2983325},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Goldstein, Spiegelhalter - 1996 - League tables and their limitations Statistical issues in comparisons of institutional performance.pdf:pdf},
isbn = {09641998},
issn = {0964-1998},
journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
keywords = {MARKOV CHAIN MONTE CARLO METHODS,MULTILEVEL MODELLING,Markov Chain Monte Carlo Methods,Multilevel Modelling,PHYSICIAN PROFILING,Physician Profiling,RANKING,RISK SHRINKAGE ESTIMATORS,Ranking,Risk Shrinkage Estimators,STRATIFICATION,Stratification,VALUE ADDED,Value Added},
number = {3},
pages = {385--443},
pmid = {845},
title = {{League tables and their limitations: Statistical issues in comparisons of institutional performance}},
volume = {159},
year = {1996}
}
@article{Flury2000,
abstract = {Suppose survival times follow an exponential distribution, and some observations are right-censored: in this situation the EM algorithm gives a straightforward solution to the problem of maximum likelihood estimation. But what happens if survival times are also left-censored, or if they follow a uniform distribution? The EM algorithm is a generic device useful in a variety of problems with incomplete data, and it appears more and more often in statistical textbooks. This article presents two exercises, which are extensions of a well-known example used in introductions to the EM algorithm. They focus on two points: the applicability of the algorithm and its self-consistency property.},
author = {Flury, Bernard and Zopp{\`{e}}, Alice},
doi = {10.1080/00031305.2000.10474546},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Flury, Zopp{\`{e}} - 2000 - Exercises in EM.pdf:pdf},
issn = {15372731},
journal = {American Statistician},
keywords = {EM,Instructional technique,Pedagogy,Statistical education,censor,censoring,exponential},
mendeley-tags = {EM,censor,censoring,exponential},
number = {3},
pages = {207--209},
title = {{Exercises in EM}},
volume = {54},
year = {2000}
}
@article{Batrinca2014,
abstract = {This paper is written for (social science) researchers seeking to analyze the wealth of social media now available. It presents a comprehensive review of software tools for social networking media, wikis, really simple syndication feeds, blogs, newsgroups, chat and news feeds. For completeness, it also includes introductions to social media scraping, storage, data cleaning and sentiment analysis. Although principally a review, the paper also provides a methodology and a critique of social media tools. Analyzing social media, in particular Twitter feeds for sentiment analysis, has become a major research and business activity due to the availability of web-based application programming interfaces (APIs) provided by Twitter, Facebook and News services. This has led to an ‘explosion' of data services, software tools for scraping and analysis and social media analytics platforms. It is also a research area undergoing rapid change and evolution due to commercial pressures and the potential for using social media data for computational (social science) research. Using a simple taxonomy, this paper provides a review of leading software tools and how to use them to scrape, cleanse and analyze the spectrum of social media. In addition, it discussed the requirement of an experimental computational environment for social media research and presents as an illustration the system architecture of a social media (analytics) platform built by University College London. The principal contribution of this paper is to provide an overview (including code fragments) for scientists seeking to utilize social media scraping and analytics either in their research or business. The data retrieval techniques that are presented in this paper are valid at the time of writing this paper (June 2014), but they are subject to change since social media data scraping APIs are rapidly changing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Batrinca, Bogdan and Treleaven, Philip C.},
doi = {10.1007/s00146-014-0549-4},
eprint = {arXiv:1011.1669v3},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Batrinca, Treleaven - 2014 - Social media analytics a survey of techniques, tools and platforms.pdf:pdf},
isbn = {0951-5666},
issn = {14355655},
journal = {AI and Society},
keywords = {Behavior economics,NLP,Opinion mining,Scraping,Sentiment analysis,Social media,Software platforms,Toolkits},
number = {1},
pages = {89--116},
pmid = {25246403},
title = {{Social media analytics: a survey of techniques, tools and platforms}},
volume = {30},
year = {2014}
}
@article{Silver2017,
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent},
doi = {10.1038/nature24270},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2017 - Article Mastering the game of Go without human knowledge.pdf:pdf},
issn = {0028-0836},
journal = {Nature Publishing Group},
number = {7676},
pages = {354--359},
publisher = {Nature Publishing Group},
title = {{Article Mastering the game of Go without human knowledge}},
url = {http://dx.doi.org/10.1038/nature24270},
volume = {550},
year = {2017}
}
@article{Kaur2016,
abstract = {-With the advent of the technology it is possible to analyze the crime which takes place in the various regions of the country. The survey of the various technologies used for this purpose is done in this paper. The technology will help in careful investigation of the crime and group them in the form of a cluster. By analyzing the crime models forecasting can be done and the impact of the crime in various regions can be stopped. There are various data development and acquisition tools available for this purpose. Various papers use various techniques for the data mining. The crimes can be partitioned into several parts. Some crimes are soft in nature and some crimes are hard in nature. In this paper all types of crime data mining techniques are analyzed and comprehensive comparison is presented.},
author = {Kaur, Navjot},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Kaur - 2016 - Data Mining Techniques used in Crime Analysis-A Review.pdf:pdf},
issn = {2395-0072},
keywords = {Crime,Data Collection,Data Mining,cluster},
pages = {1981--1984},
title = {{Data Mining Techniques used in Crime Analysis:-A Review}},
volume = {08},
year = {2016}
}
@article{Lawrence2017,
abstract = {Application of models to data is fraught. Data-generating collaborators often only have a very basic understanding of the complications of collating, processing and curating data. Challenges include: poor data collection practices, missing values, inconvenient storage mechanisms, intellectual property, security and privacy. All these aspects obstruct the sharing and interconnection of data, and the eventual interpretation of data through machine learning or other approaches. In project reporting, a major challenge is in encapsulating these problems and enabling goals to be built around the processing of data. Project overruns can occur due to failure to account for the amount of time required to curate and collate. But to understand these failures we need to have a common language for assessing the readiness of a particular data set. This position paper proposes the use of data readiness levels: it gives a rough outline of three stages of data preparedness and speculates on how formalisation of these levels into a common language for data readiness could facilitate project management.},
archivePrefix = {arXiv},
arxivId = {1705.02245},
author = {Lawrence, Neil D.},
eprint = {1705.02245},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Lawrence - 2017 - Data Readiness Levels.pdf:pdf},
keywords = {()},
number = {April},
pages = {1--10},
title = {{Data Readiness Levels}},
url = {http://arxiv.org/abs/1705.02245},
year = {2017}
}
@book{Hastie2001,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {1010.3003},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {The Mathematical Intelligencer},
doi = {10.1198/jasa.2004.s339},
eprint = {1010.3003},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - 2001 - The Elements of Statistical Learning.pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {03436993},
keywords = {inger series in statistics},
number = {2},
pages = {83--85},
pmid = {21196786},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf{\%}255Cnhttp://www-stat.stanford.edu/{~}tibs/book/preface.ps},
volume = {27},
year = {2001}
}
@book{Goodfellow2016,
abstract = {Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Nature},
doi = {10.1038/nmeth.3707},
eprint = {arXiv:1312.6184v5},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow, Bengio, Courville - 2016 - Deep Learning.pdf:pdf},
isbn = {978-0262035613},
issn = {0028-0836},
number = {7553},
pages = {800},
pmid = {26017442},
title = {{Deep Learning}},
url = {http://goodfeli.github.io/dlbook/{\%}0Ahttp://dx.doi.org/10.1038/nature14539},
volume = {521},
year = {2016}
}
@book{Barber2011,
abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
annote = {For Lecture 1, read chapters 1, 8, 9, 13


For Lecture 2, read chapters 17, 18

For Lecture 3, read chapters 18.2},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Barber, David},
doi = {10.1017/CBO9780511804779},
eprint = {arXiv:1011.1669v3},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Barber - 2011 - Bayesian Reasoning and Machine Learning.pdf:pdf},
isbn = {9780511804779},
issn = {9780521518147},
pmid = {16931139},
title = {{Bayesian Reasoning and Machine Learning}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511804779},
year = {2011}
}
@book{Bishop2007,
abstract = {$\backslash$nThis book provides an introduction to the field of pattern recognition and machine learning. It gives an overview of several basic and advanced topics in machine learning theory. The book is definitely valuable to scientists and engineers who are involved in developing machine learning tools applied to signal and image processing applications. This book is also suitable for courses on machine learning and pattern recognition, designed for advanced undergraduates or PhD students. No previous knowledge of machine learning concepts or algorithms is assumed, but readers need some knowledge of calculus and linear algebra. The book is complemented by a great deal of additional supports for instructors and students. The supports include solutions to the exercises in each chapter, the example data sets used throughout the book and the forthcoming companion book that deals with practical and software implementations of the key algorithms. A strong point of this book is that the mathematical expressions or algorithms are usually accompanied with colorful graphs and figures. This definitely helps to communicate the concepts much better to the students or the interested researchers than pure description of the algorithms. The book also provides an interesting short biography of the key scientists and mathematicians who have contributed historically to the basic mathematical concepts and methods in each chapter.$\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher M},
booktitle = {Journal of Electronic Imaging},
doi = {10.1117/1.2819119},
eprint = {arXiv:1011.1669v3},
file = {:Users/jp2011/Library/Application Support/Mendeley Desktop/Downloaded/Bishop - 2007 - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {978-0-387-31073-2},
issn = {1017-9909},
number = {4},
pages = {049901},
pmid = {25246403},
title = {{Pattern Recognition and Machine Learning}},
url = {http://electronicimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.2819119},
volume = {16},
year = {2007}
}
